from sine.agents.storm.prompts import (REFINE_OUTLINE, WRITE_DRAFT_OUTLINE,
                                       WRITE_SECTION)
from sine.agents.storm.utils import (clean_up_outline,
                                     limit_word_count_preserve_newline,
                                     process_table_of_contents)
from sine.common.logger import logger


class OutlineWriter:
    """Write draft outline first and then improve based on conversation and
    draft outline."""

    def __init__(self, writer_engine) -> None:
        self.llm = writer_engine

    def write_draft_outline(self, topic):
        message = [
            dict(role="user", content=WRITE_DRAFT_OUTLINE.format(topic=topic)),
        ]

        response = self.llm.chat(message)

        return clean_up_outline(response)

    def _format_conversation(self, conversation_history):
        # format chat history to conversation string
        conversation_str = "\n"

        for conversations in conversation_history.values():
            for turn in conversations:
                if turn["role"] == "user":
                    conversation_str += f"Wikipedia writer: {turn['content']}\n"
                else:
                    conversation_str += f"Expert: {turn['content']}\n"

        return conversation_str

    def refine_outline(self, topic, draft_outline, conversation):
        message = [
            dict(
                role="user",
                content=REFINE_OUTLINE.format(
                    topic=topic, conversation=conversation, draft_outline=draft_outline
                ),
            ),
        ]

        return self.llm.chat(message)

    def write(self, topic, chat_history):
        # step 1: write draft first
        draft_outline = self.write_draft_outline(topic)
        logger.info(f"Draft outline (directly generated by llm):\n {draft_outline}")

        # step 2: using converations to refine the directly generated outline
        # format conversation
        conversation = self._format_conversation(chat_history)
        # limit the conversation tokens as the api model has upper limit token per minute
        conversation = limit_word_count_preserve_newline(conversation, max_word_count=3500)

        # improve outline
        outline = self.refine_outline(topic, draft_outline, conversation)
        outline = clean_up_outline(outline)
        logger.info(f"Refined outline (improved by conversation):\n {outline}")

        return outline

class ArticleWriter:
    '''ArticleWriter write section by section.'''

    def __init__(self, writer_engine) -> None:
        self.llm = writer_engine

    def _format_snippet(self, snippets):
        info = ''
        for n, r in enumerate(snippets):
            info += f'[{n + 1}] ' + '\n'.join([r])
            info += '\n\n'
        return info

    def write_section(self, topic, section_title, section_retrievals, sub_section_outline = None):
        """Section writer writes the content of each section based on retrievals and section outline.
        
        NOTE: The section writer only writes the first level sections, sub-section titles are used for
        retrieving information from search results, and the subsections generated are not following
        strictly the generated outline in previous steps. But you could customized to generate following
        subsection outlines.
        See the issue for detail reason: `https://github.com/stanford-oval/storm/issues/30`
        
        Args:
            topic (str): the topic of this article
            section_retrievals (str): the information retrieved from the subsection titles using vector search
            sub_section_outline (str): the subsection outline string in markdown format (e.g. ##subtitles)

        """

        message = [
            dict(role='user',
                 content=WRITE_SECTION.format(
                     info=section_retrievals,
                     topic=topic,
                     section_title=section_title)),
        ]

        response = self.llm.chat(message)

        return response

    def write(self, topic, outline, vector_db):
        """ Write the article section by section.

        Args:
            topic (str): topic of interest
            outline (str): outline of the article, with markdown hash tags,
                            e.g. #, ## indicating section and subsections etc
            vector_db (str): search section related info from vector_db

        TODO: use concurrent.futures.ThreadPoolExecutor to make it parallel,
        but mind the rate limit of the API.
        """
        outline_tree = process_table_of_contents(outline)
        outline_tree = list(outline_tree.values())[0]

        article = []
        for section_title in outline_tree:
            logger.info(f"Writing section: {section_title}")
            retrievals = vector_db.search([section_title], top_k = 10)
            section_content = self.write_section(topic, section_title, retrievals)
            article.append(section_content)

        article_md_str = ''
        for section in article:
            article_md_str += section

        return article, article_md_str
